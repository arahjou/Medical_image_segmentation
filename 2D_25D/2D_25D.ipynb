{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6f79cd3aac44648a1bf07e99a097b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f9a10194124ea3bb67bb72facae696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1db55060fa5a4bec96d26ec968339624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess the DataFrame\n",
    "df_train = pd.read_csv(\"/Users/arahjou/Downloads/Medical_seg/Backup/train.csv\")\n",
    "df_train = df_train.sort_values([\"id\", \"class\"]).reset_index(drop=True)\n",
    "df_train[\"patient\"] = df_train.id.apply(lambda x: x.split(\"_\")[0])\n",
    "df_train[\"days\"] = df_train.id.apply(lambda x: \"_\".join(x.split(\"_\")[:2]))\n",
    "\n",
    "# Read and process all image files\n",
    "all_image_files = sorted(\n",
    "    glob.glob(\"/Users/arahjou/Downloads/Medical_seg/Backup/train/*/*/scans/*.png\"),\n",
    "    key=lambda x: x.split(\"/\")[3] + \"_\" + x.split(\"/\")[5]\n",
    ")\n",
    "size_x = [int(os.path.basename(x)[:-4].split(\"_\")[-4]) for x in all_image_files]\n",
    "size_y = [int(os.path.basename(x)[:-4].split(\"_\")[-3]) for x in all_image_files]\n",
    "spacing_x = [float(os.path.basename(x)[:-4].split(\"_\")[-2]) for x in all_image_files]\n",
    "spacing_y = [float(os.path.basename(x)[:-4].split(\"_\")[-1]) for x in all_image_files]\n",
    "\n",
    "# Assign images to DataFrame\n",
    "df_train[\"image_files\"] = np.repeat(all_image_files, 3)\n",
    "df_train[\"spacing_x\"] = np.repeat(spacing_x, 3)\n",
    "df_train[\"spacing_y\"] = np.repeat(spacing_y, 3)\n",
    "df_train[\"size_x\"] = np.repeat(size_x, 3)\n",
    "df_train[\"size_y\"] = np.repeat(size_y, 3)\n",
    "df_train[\"slice\"] = np.repeat([int(os.path.basename(x)[:-4].split(\"_\")[-5]) for x in all_image_files], 3)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_val, test = train_test_split(df_train['days'].unique(), test_size=0.05, random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=0.20 / 0.95, random_state=42)  # Adjust proportion to account for earlier split\n",
    "\n",
    "# Helper function to decode RLE masks\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = np.array(mask_rle.split(), dtype=int)\n",
    "    starts, lengths = s[0::2] - 1, s[1::2]\n",
    "    ends = starts + lengths\n",
    "    h, w = shape\n",
    "    img = np.zeros((h * w,), dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# Create directories for each set\n",
    "!mkdir -p ./mmseg_train/{images,labels,splits}\n",
    "!mkdir -p ./mmseg_val/{images,labels,splits}\n",
    "!mkdir -p ./mmseg_test/{images,labels,splits}\n",
    "\n",
    "# Function to process groups of data for 2D images\n",
    "def process_data(groups, set_name):\n",
    "    for day, group in tqdm(groups):\n",
    "        patient = group.patient.iloc[0]\n",
    "        for file_name in group.image_files.unique():\n",
    "            img = cv2.imread(file_name, cv2.IMREAD_ANYDEPTH)\n",
    "            segms = group.loc[group.image_files == file_name]\n",
    "            masks = {}\n",
    "            for segm, label in zip(segms.segmentation, segms[\"class\"]):\n",
    "                if not pd.isna(segm):\n",
    "                    mask = rle_decode(segm, img.shape[:2])\n",
    "                    masks[label] = mask\n",
    "                else:\n",
    "                    masks[label] = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "            mask_stack = np.stack([masks[k] for k in sorted(masks)], -1)\n",
    "            new_file_name = f\"{day}_{os.path.basename(file_name)}\"\n",
    "            cv2.imwrite(f\"./{set_name}/images/{new_file_name}\", img)\n",
    "            cv2.imwrite(f\"./{set_name}/labels/{new_file_name}\", mask_stack)\n",
    "\n",
    "# Process each set\n",
    "process_data(df_train[df_train['days'].isin(train)].groupby('days'), 'mmseg_train')\n",
    "process_data(df_train[df_train['days'].isin(val)].groupby('days'), 'mmseg_val')\n",
    "process_data(df_train[df_train['days'].isin(test)].groupby('days'), 'mmseg_test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing images with no related run-length values\n",
    "\n",
    "To modify the code to remove images for which there is no run-length encoding (RLE) value in the CSV, you can add a filtering step before processing the images. This step will exclude any rows from the DataFrame where the 'segmentation' column is NaN (Not a Number), which represents the absence of an RLE value.\n",
    "\n",
    "Here's the revised version of your code with the necessary adjustments to ensure that images with no corresponding segmentation data are not processed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load and preprocess the DataFrame\n",
    "df_train = pd.read_csv(\"/Users/arahjou/Downloads/Medical_seg/Backup/train.csv\")\n",
    "df_train = df_train.sort_values([\"id\", \"class\"]).reset_index(drop=True)\n",
    "df_train[\"patient\"] = df_train.id.apply(lambda x: x.split(\"_\")[0])\n",
    "df_train[\"days\"] = df_train.id.apply(lambda x: \"_\".join(x.split(\"_\")[:2]))\n",
    "\n",
    "# Filter out entries without a run-length encoding\n",
    "df_train = df_train.dropna(subset=['segmentation'])\n",
    "\n",
    "# Read and process all image files\n",
    "all_image_files = sorted(\n",
    "    glob.glob(\"/Users/arahjou/Downloads/Medical_seg/Backup/train/*/*/scans/*.png\"),\n",
    "    key=lambda x: x.split(\"/\")[3] + \"_\" + x.split(\"/\")[5]\n",
    ")\n",
    "size_x = [int(os.path.basename(x)[:-4].split(\"_\")[-4]) for x in all_image_files]\n",
    "size_y = [int(os.path.basename(x)[:-4].split(\"_\")[-3]) for x in all_image_files]\n",
    "spacing_x = [float(os.path.basename(x)[:-4].split(\"_\")[-2]) for x in all_image_files]\n",
    "spacing_y = [float(os.path.basename(x)[:-4].split(\"_\")[-1]) for x in all_image_files]\n",
    "\n",
    "# Assign images to DataFrame\n",
    "df_train[\"image_files\"] = np.repeat(all_image_files, 3)\n",
    "df_train[\"spacing_x\"] = np.repeat(spacing_x, 3)\n",
    "df_train[\"spacing_y\"] = np.repeat(spacing_y, 3)\n",
    "df_train[\"size_x\"] = np.repeat(size_x, 3)\n",
    "df_train[\"size_y\"] = np.repeat(size_y, 3)\n",
    "df_train[\"slice\"] = np.repeat([int(os.path.basename(x)[:-4].split(\"_\")[-5]) for x in all_image_files], 3)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_val, test = train_test_split(df_train['days'].unique(), test_size=0.05, random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=0.20 / 0.95, random_state=42)\n",
    "\n",
    "# Helper function to decode RLE masks\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = np.array(mask_rle.split(), dtype=int)\n",
    "    starts, lengths = s[0::2] - 1, s[1::2]\n",
    "    ends = starts + lengths\n",
    "    h, w = shape\n",
    "    img = np.zeros((h * w,), dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# Create directories for each set\n",
    "os.makedirs(\"./mmseg_train/images\", exist_ok=True)\n",
    "os.makedirs(\"./mmseg_train/labels\", exist_ok=True)\n",
    "os.makedirs(\"./mmseg_val/images\", exist_ok=True)\n",
    "os.makedirs(\"./mmseg_val/labels\", exist_ok=True)\n",
    "os.makedirs(\"./mmseg_test/images\", exist_ok=True)\n",
    "os.makedirs(\"./mmseg_test/labels\", exist_ok=True)\n",
    "\n",
    "# Function to process groups of data for 2D images\n",
    "def process_data(groups, set_name):\n",
    "    for day, group in groups:\n",
    "        patient = group.patient.iloc[0]\n",
    "        for file_name in group.image_files.unique():\n",
    "            img = cv2.imread(file_name, cv2.IMREAD_ANYDEPTH)\n",
    "            segms = group.loc[group.image_files == file_name]\n",
    "            masks = {}\n",
    "            for segm, label in zip(segms.segmentation, segms[\"class\"]):\n",
    "                mask = rle_decode(segm, img.shape[:2])\n",
    "                masks[label] = mask\n",
    "            mask_stack = np.stack([masks[k] for k in sorted(masks)], -1)\n",
    "            new_file_name = f\"{day}_{os.path.basename(file_name)}\"\n",
    "            cv2.imwrite(f\"./{set_name}/images/{new_file_name}\", img)\n",
    "            cv2.imwrite(f\"./{set_name}/labels/{new_file_name}\", mask_stack)\n",
    "\n",
    "# Process each set\n",
    "process_data(df_train[df_train['days'].isin(train)].groupby('days'), 'mmseg_train')\n",
    "process_data(df_train[df_train['days'].isin(val)].groupby('days'), 'mmseg_val')\n",
    "process_data(df_train[df_train['days'].isin(test)].groupby('days'), 'mmseg_test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39e3f750f314b9d84fd4ec28991f44b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b572d6d7d675437ea3d5d269c2a26d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b189a8cbc2647f29d4f262659854c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load and preprocess the DataFrame\n",
    "df_train = pd.read_csv(\"/Users/arahjou/Downloads/Medical_seg/Backup/train.csv\")\n",
    "df_train = df_train.sort_values([\"id\", \"class\"]).reset_index(drop=True)\n",
    "df_train[\"patient\"] = df_train.id.apply(lambda x: x.split(\"_\")[0])\n",
    "df_train[\"days\"] = df_train.id.apply(lambda x: \"_\".join(x.split(\"_\")[:2]))\n",
    "\n",
    "# Read and process all image files\n",
    "all_image_files = sorted(\n",
    "    glob.glob(\"/Users/arahjou/Downloads/Medical_seg/Backup/train/*/*/scans/*.png\"),\n",
    "    key=lambda x: x.split(\"/\")[3] + \"_\" + x.split(\"/\")[5]\n",
    ")\n",
    "size_x = [int(os.path.basename(x)[:-4].split(\"_\")[-4]) for x in all_image_files]\n",
    "size_y = [int(os.path.basename(x)[:-4].split(\"_\")[-3]) for x in all_image_files]\n",
    "spacing_x = [float(os.path.basename(x)[:-4].split(\"_\")[-2]) for x in all_image_files]\n",
    "spacing_y = [float(os.path.basename(x)[:-4].split(\"_\")[-1]) for x in all_image_files]\n",
    "\n",
    "# Assign images to DataFrame\n",
    "df_train[\"image_files\"] = np.repeat(all_image_files, 3)\n",
    "df_train[\"spacing_x\"] = np.repeat(spacing_x, 3)\n",
    "df_train[\"spacing_y\"] = np.repeat(spacing_y, 3)\n",
    "df_train[\"size_x\"] = np.repeat(size_x, 3)\n",
    "df_train[\"size_y\"] = np.repeat(size_y, 3)\n",
    "df_train[\"slice\"] = np.repeat([int(os.path.basename(x)[:-4].split(\"_\")[-5]) for x in all_image_files], 3)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_val, test = train_test_split(df_train['days'].unique(), test_size=0.05, random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=0.20 / 0.95, random_state=42)  # Adjust proportion to account for earlier split\n",
    "\n",
    "# Helper function to decode RLE masks\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = np.array(mask_rle.split(), dtype=int)\n",
    "    starts, lengths = s[0::2] - 1, s[1::2]\n",
    "    ends = starts + lengths\n",
    "    h, w = shape\n",
    "    img = np.zeros((h * w,), dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# Create directories for each set\n",
    "!mkdir -p ./mmseg_train/{images,labels,splits}\n",
    "!mkdir -p ./mmseg_val/{images,labels,splits}\n",
    "!mkdir -p ./mmseg_test/{images,labels,splits}\n",
    "\n",
    "def process_data(groups, set_name, target_size=(256, 256)):  # Example target size\n",
    "    for day, group in tqdm(groups):\n",
    "        patient = group.patient.iloc[0]\n",
    "        imgs = []\n",
    "        msks = []\n",
    "        for file_name in group.image_files.unique():\n",
    "            img = cv2.imread(file_name, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, target_size)  # Resize image to target size\n",
    "            segms = group.loc[group.image_files == file_name]\n",
    "            masks = {}\n",
    "            for segm, label in zip(segms.segmentation, segms[\"class\"]):\n",
    "                if not pd.isna(segm):\n",
    "                    mask = rle_decode(segm, img.shape)\n",
    "                    masks[label] = mask\n",
    "                else:\n",
    "                    masks[label] = np.zeros(img.shape, dtype=np.uint8)\n",
    "            masks = np.stack([masks[k] for k in sorted(masks)], -1)\n",
    "            imgs.append(img)\n",
    "            msks.append(masks)\n",
    "\n",
    "        imgs = np.stack(imgs, 0)\n",
    "        msks = np.stack(msks, 0)\n",
    "        for i in range(msks.shape[0]):\n",
    "            img_slice_range = imgs[max(0, i - 2):min(imgs.shape[0], i + 3)]\n",
    "            img = np.mean(img_slice_range, axis=0)  # Average to maintain one channel\n",
    "            msk = msks[i]\n",
    "            new_file_name = f\"{day}_{i}.png\"\n",
    "            cv2.imwrite(f\"./{set_name}/images/{new_file_name}\", img.astype(np.uint8))\n",
    "            cv2.imwrite(f\"./{set_name}/labels/{new_file_name}\", msk[:, :, 0].astype(np.uint8))  # Assuming mask is single-channel\n",
    "\n",
    "\n",
    "# Process each set\n",
    "process_data(df_train[df_train['days'].isin(train)].groupby('days'), 'mmseg_train')\n",
    "process_data(df_train[df_train['days'].isin(val)].groupby('days'), 'mmseg_val')\n",
    "process_data(df_train[df_train['days'].isin(test)].groupby('days'), 'mmseg_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing images with no related run-length values\n",
    "To modify the given code to exclude images without a run-length encoding (RLE) value for their segmentation data, you need to filter out such entries before proceeding with image processing and data assignment. Here's how you can integrate this filtering step into your existing script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://idiotdeveloper.com/step-by-step-guide-to-resnet50-unet-in-tensorflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Load and preprocess the DataFrame\n",
    "df_train = pd.read_csv(\"/Users/arahjou/Downloads/Medical_seg/Backup/train.csv\")\n",
    "df_train = df_train.sort_values([\"id\", \"class\"]).reset_index(drop=True)\n",
    "df_train[\"patient\"] = df_train.id.apply(lambda x: x.split(\"_\")[0])\n",
    "df_train[\"days\"] = df_train.id.apply(lambda x: \"_\".join(x.split(\"_\")[:2]))\n",
    "\n",
    "# Filter out entries without a run-length encoding\n",
    "df_train = df_train.dropna(subset=['segmentation'])\n",
    "\n",
    "# Read and process all image files\n",
    "all_image_files = sorted(\n",
    "    glob.glob(\"/Users/arahjou/Downloads/Medical_seg/Backup/train/*/*/scans/*.png\"),\n",
    "    key=lambda x: x.split(\"/\")[3] + \"_\" + x.split(\"/\")[5]\n",
    ")\n",
    "size_x = [int(os.path.basename(x)[:-4].split(\"_\")[-4]) for x in all_image_files]\n",
    "size_y = [int(os.path.basename(x)[:-4].split(\"_\")[-3]) for x in all_image_files]\n",
    "spacing_x = [float(os.path.basename(x)[:-4].split(\"_\")[-2]) for x in all_image_files]\n",
    "spacing_y = [float(os.path.basename(x)[:-4].split(\"_\")[-1]) for x in all_image_files]\n",
    "\n",
    "# Assign images to DataFrame\n",
    "df_train[\"image_files\"] = np.repeat(all_image_files, 3)\n",
    "df_train[\"spacing_x\"] = np.repeat(spacing_x, 3)\n",
    "df_train[\"spacing_y\"] = np.repeat(spacing_y, 3)\n",
    "df_train[\"size_x\"] = np.repeat(size_x, 3)\n",
    "df_train[\"size_y\"] = np.repeat(size_y, 3)\n",
    "df_train[\"slice\"] = np.repeat([int(os.path.basename(x)[:-4].split(\"_\")[-5]) for x in all_image_files], 3)\n",
    "\n",
    "# Split data into training, validation, and test sets\n",
    "train_val, test = train_test_split(df_train['days'].unique(), test_size=0.05, random_state=42)\n",
    "train, val = train_test_split(train_val, test_size=0.20 / 0.95, random_state=42)\n",
    "\n",
    "# Helper function to decode RLE masks\n",
    "def rle_decode(mask_rle, shape):\n",
    "    s = np.array(mask_rle.split(), dtype=int)\n",
    "    starts, lengths = s[0::2] - 1, s[1::2]\n",
    "    ends = starts + lengths\n",
    "    h, w = shape\n",
    "    img = np.zeros((h * w,), dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# Create directories for each set\n",
    "os.makedirs(\"./mmseg_train/images\", exist_ok=True)\n",
    "os.makedirs(\"./mmseg_train/labels\", exist_ok=True)\n",
    "os.makedirs(\"./mmseg_val/images\", exist_ok=True)\n",
    "os.makedirs(\"./mmseg_val/labels\", exist_ok=True)\n",
    "os.makedirs(\"./mmseg_test/images\", exist_ok=True)\n",
    "os.makedirs(\"./mmseg_test/labels\", exist_ok=True)\n",
    "\n",
    "def process_data(groups, set_name, target_size=(256, 256)):  # Example target size\n",
    "    for day, group in tqdm(groups):\n",
    "        patient = group.patient.iloc[0]\n",
    "        imgs = []\n",
    "        msks = []\n",
    "        for file_name in group.image_files.unique():\n",
    "            img = cv2.imread(file_name, cv2.IMREAD_GRAYSCALE)\n",
    "            img = cv2.resize(img, target_size)  # Resize image to target size\n",
    "            segms = group.loc[group.image_files == file_name]\n",
    "            masks = {}\n",
    "            for segm, label in zip(segms.segmentation, segms[\"class\"]):\n",
    "                mask = rle_decode(segm, img.shape)\n",
    "                masks[label] = mask\n",
    "            masks = np.stack([masks[k] for k in sorted(masks)], -1)\n",
    "            imgs.append(img)\n",
    "            msks.append(masks)\n",
    "\n",
    "        imgs = np.stack(imgs, 0)\n",
    "        msks = np.stack(msks, 0)\n",
    "        for i in range(msks.shape[0]):\n",
    "            img_slice_range = imgs[max(0, i - 2):min(imgs.shape[0], i + 3)]\n",
    "            img = np.mean(img_slice_range, axis=0)  # Average to maintain one channel\n",
    "            msk = msks[i]\n",
    "            new_file_name = f\"{day}_{i}.png\"\n",
    "            cv2.imwrite(f\"./{set_name}/images/{new_file_name}\", img.astype(np.uint8))\n",
    "            cv2.imwrite(f\"./{set_name}/labels/{new_file_name}\", msk[:, :, 0].astype(np.uint8))  # Assuming mask is single-channel\n",
    "\n",
    "# Process each set\n",
    "process_data(df_train[df_train['days'].isin(train)].groupby('days'), 'mmseg_train')\n",
    "process_data(df_train[df_train['days'].isin(val)].groupby('days'), 'mmseg_val')\n",
    "process_data(df_train[df_train['days'].isin(test)].groupby('days'), 'mmseg_test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
